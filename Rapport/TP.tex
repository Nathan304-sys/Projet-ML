\documentclass[10pt,a4paper]{article}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{makeidx}
\usepackage{graphicx}
\usepackage{float}
\usepackage{color}
\usepackage[a4paper, margin=2cm]{geometry}
\usepackage{fancyhdr}  % Charger fancyhdr
\pagestyle{fancy}      % Activer fancyhdr
\renewcommand{\footrulewidth}{0.4pt}
\begin{document}
	
	
		\thispagestyle{empty}
		%\pagenumbering{gobble}  % Supprime la numérotation de la première page
	\begin{center}
		{\includegraphics[scale=0.1]{40ansISSEA}~\begin{tabular}{c}\large\textbf{ Communauté Economique et Monetaire }\\ 
				\large\textbf{ d'Afrique Centrale } \\		
				%\\ \;\;
				%\rule{0.06\textwidth}{4pt} \footnotesize \textbf{FACULTÉ DES SCIENCES ET TECHNIQUES} \rule{0.06\textwidth}{4pt}
			\end{tabular} \includegraphics[scale=0.1]{40ansISSEA} 
			
		}
	\end{center}
	
	\[\begin{array}{lcc}
		\mbox{\small \textbf{\underline {BP:} }294,Yaoundé(République de Cameroun)}	&  \mbox{\textbf{\underline{Tél:} } (+237)22-22-01-34}& \mbox {\textbf{\underline{Fax:} }(+237)22-22-95-21 }  \\
		\mbox{\textbf{\underline{Email:} }Isseacemac@yahoo.fr}& \mbox{\textbf{\underline{ Web:}} www.issea-cemac.org}  & \mbox{\textbf{\underline{Année : }} Janvier 2024-2025}\\
	\end{array}\]
	
	
	\vspace*{2cm}
	\begin{center}
		{\Large \textbf{ PROJET DE MACHINE LEARNING ET OPTIMISATION}}\\
		
	\end{center}
	
	\begin{center}
		Rédigé par: \\
		{{\textbf{{\begin{itemize}
							\item ANATO Diane
								\item NSIMOUESSA Dieuveil Nathan
		\end{itemize}}}}} \vspace*{2cm}
		%\small {{ELEVES-INGENIEURS, ISE 2 \\ GROUPE 6}}
		
		\rule{1 \textwidth}{4pt} \\\vspace*{0.5cm}
		{ \large{\color{blue}{\textbf{LA METHODE DE NEWTON CONVERGE TOUJOURS PLUS RAPIDEMENT QUE LA DESCENTE DE GRADIENT?}}}} \\ \vspace*{0.5cm}
		\rule{1 \textwidth}{4pt} 
		\\\vspace*{1cm}
		\textit{\underline{\large\textbf{Sous la supervision de}}:Mme Marie Thérèse MBIA NDI, Ingénieur Statisticien Economiste }
	\end{center}
	
	%\pagenumbering{roman}    
	%\thispagestyle{empty} 
	%\newpage
	% Modifier l'en-tête et le pied de page
	\fancyhead[L]{}  % En-tête gauche
	\fancyhead[C]{La Methode de Newthon converge plus rapidement que la méthode de Newton? }   % En-tête centre
	\fancyhead[R]{Page \thepage} % En-tête droite avec numéro de page
	\fancyfoot[LE, RO]{ \scriptsize\textbf{Janvier 2025 }}
	\fancyfoot[LO]{\scriptsize \emph \textbf{{NSIMOUESSA Dieuveil \& ANATO Diane}}}
	\fancyfoot[CO]{\scriptsize \textbf{Machine learning et optimisation }}	
	%\fancyfoot[L]{{Projet ML optimisation}  % Pied de page gauche
	%\fancyfoot[C]{Janvier 2024}  % Pied de page centre
	%\fancyfoot[R]{\thepage}      % Pied de page droite (numéro de page)
	
\newpage	
\pagenumbering{arabic}
\section*{Introduction}

Dans le domaine de l'optimisation, la recherche de solutions rapides et précises est primordiale. La méthode de Newton et la descente de gradient sont deux outils couramment utilisés pour atteindre cet objectif. Bien que toutes deux visent à minimiser une fonction, leurs approches diffèrent significativement. Nous nous proposons d'analyser en détail ces différences et de déterminer si la méthode de Newton conserve toujours son avantage en termes de vitesse de convergence. 
	
\section{Généralité sur la méthode de Newton et la méthode de descente de gradient}
\subsection{La méthode de Newton}

La méthode de Newton, initialement conçue pour trouver les zéros d'une fonction, peut être adaptée pour résoudre des problèmes d'optimisation. L'idée de base est de chercher les points où la dérivée d'une fonction s'annule, car ces points correspondent généralement à des extrema (minimum ou maximum) de la fonction.\\
Considérons une fonction \(f(x)\) que l'on souhaite minimiser. On calcule le gradient de f(x), noté , qui représente la direction de plus forte pente de la fonction en un point donné. En pratique, on part d'un point de départ \( x_0\) et on calcule la nouvelle estimation \(x_1\) en utilisant la formule :
\begin{equation}
 x_{k+1} = x_{k} - H^{-1}(x_k) \cdot \nabla f(x_k)
\end{equation}
où :	\(H^{-1}(x_k)\) est l'inverse de la matrice Hessienne évaluée en \(x_k\) k $\in \{0...n\}$

Nous rappelons que la méthode de Newton est utilisée sur les fonctions deux fois différentiables et où la matrice hésienne est inversible.

\subsection{Méthode de descente de gradient}
La méthode de descente de gradient est une technique d'optimisation utilisée pour minimiser une fonction en ajustant progressivement ses paramètres dans la direction du gradient opposé. Elle est appliquée sur les fonctions convexes et différentiables. 
L'algorithme de cette méthode est la suivante :
\begin{equation}
x_{k+1} = x_k - \alpha \cdot \nabla f(x_k)
\end{equation}
où : \(\alpha\) est le taux d'apprentissage et \(\nabla f(x_k)\) est le gradient de la fonction \(f\) évalué en \(x_k\).

	
\subsection{Comparaison de Convergence}
La méthode de Newton converge rapidement si la fonction est deux fois continûment différentiable et si on commence suffisamment près d'un minimum local. Si ce n'est pas le cas, la méthode peut échouer, diverger ou être inefficace. La descente de gradient, quant à elle, est plus robuste et peut converger dans des situations où la méthode de Newton échoue, mais elle est souvent plus lente.\\
Par ailleurs, la méthode de Newton nécessite le calcul et l'inversion de la matrice hessienne (matrice des dérivées secondes), ce qui est coûteux pour les problèmes de grande dimension. Dans ce cas, la descente de gradient peut être plus pratique et efficace.

\section{Présentation des résultats}
Afin de résoudre le problème soumis, nous avons utilisées 3 bases réelles (Titanic, Boston, Digit )tous provenant respectivement de Kaggle et Scikit-learn afin de voir la méthode qui converge le plus rapidement selon les cas .

\subsection{Le dataset de titanic}
Le dataset Titanic est un jeu de données emblématique en science des données, utilisé pour l'analyse exploratoire et les modèles de classification. Il regroupe des informations sur les passagers du Titanic comprenant 1309 lignes et 12 colonnes, notamment leur âge, leur sexe, leur classe de voyage, le prix de leur billet et leur port d'embarquement. Il sera question de minimiser la fonction coût qui est l'opposée de la fonction de vraisemblance de prédiction du statut d'un individu (décès ou survivant).a\\
Après traitement de la base de donnée, on remarque qu'il y a plus de décès que de survivant soit 62,3\%. Parmi les décès, il y a plus d'hommes que de femmes. Cependant, on observe moins d'hommes survivants que de femmes survivantes. 
\begin{figure}[H]
 	\centering
 	\caption{Statistique descriptive sur le dataset Titanic.}
 	\includegraphics[width=0.9\textwidth]{Stat1}
 	\label{fig:Stat}
 	\text{\underline{Source} : kaggle}
\end{figure} 
%\vspace*{0.3cm}

L'implémentation de la méthode de la descente gradient et celle de Newton avec une fonction objective logistique (Figure~\ref{fig:Conv1}) révèle qu'à la \textbf{deuxième itération}, la méthode Newton converge ce qui n'est pas le cas de la méthode de la descente de gradient où la convergence s'observe après \textbf{100 itérations avec un taux d'apprentissage $\gamma= 0,1$ } \\
\begin{figure}[h]
	\centering
	\caption{Vitesse de convergence sur le dataset Titanic.}
	\includegraphics[width=0.9\textwidth]{Conv1}
	\label{fig:Conv1}
	\text{\underline{Source} : kaggle}
\end{figure} 
%\vspace*{2cm}
\subsection{Le dataset de Boston}
La base de données Boston de scikit-learn est un jeu de données classique contenant 506 quartiers de la ville de Boston, utilisé pour la régression linéaire, notamment pour prédire le prix des maisons en fonction de plusieurs caractéristiques. En appliquant la méthode de Newton et descente de gradient avec une fonction objectif quadratique on obtient le résultat ci dessous (Figure \ref{fig:Conv2}).
Il contient des informations sur, avec pour objectif de prédire le prix médian des maisons en fonction de différentes 

\begin{figure}[h]
	\centering
	\caption{Vitesse de convergence sur le dataset Boston.}
	\includegraphics[width=0.9\textwidth]{Conv2.png}
	\label{fig:Conv2}
	\text{\underline{Source} : kaggle}
\end{figure} 
On note une \textbf{baisse lente de la vitesse convergence}  (erreur quadratique moyenne) pour la méthode de gradient et une \textbf{vitesse constante} pour la méthode de Newton.
\vspace*{2cm}
\subsection{Le dataset de digit}
La base de données Digits de scikit-learn est un jeu de données utilisé pour la classification d'images de chiffres manuscrits (0 à 9) contenant 1797 images de chiffres, chaque image est de taille 8x8 pixels, ce qui donne un total de 64 caractéristiques par image. En appliquant le méthode de Newton et descente de gradient sur cette base avec la fonction sigmoïde, on observe que \textbf{ la matrice Hessienne n'est pas inversible} et par conséquent la \textbf{méthode de Newton ne convergence pas} .



\begin{figure}[h]
	\centering
	\caption{Vitesse de convergence sur le dataset Boston.}
	\includegraphics[width=0.5\textwidth]{Conv3.png}
	\label{fig:Conv}
	\text{\underline{Source} : kaggle}
 \end{figure}  
 On observe seulement la convergence de Newtonne car la sklearn 
\section{Conclusion}
Il était question dans notre travail de répondre à la problématique suivant laquelle si la méthode de Newton converge toujours plus rapidement que la méthode de gradient. Pour ce faire, il était question de mettre en exergue trois bases (Titanic, Boston, Digit) en utilisant comme fonction objective une régression logistique et une fonction quadratique. Il ressort des trois résultats que 
\textbf{la méthode de Newton converge plus rapidement avec la base de données du Titanic, et ne converge pas avec la base de données digit} . Ces résultats coroborent avec la théorie d'optimisation de ces deux méthodes du fait que la méthode de Newton est efficace et converge plus vite dans les datasets de petites tailles. 
Ainsi, nous pouvons conclure que la méthode de Newton \textbf{ne converge pas toujours plus rapidement}  que la descente de gradient .
 %ne converge pas toujours plus rapidement que la méthode de la descente de gradient. En effet,
 %Bien que la méthode de Newton ait souvent une convergence plus rapide (quadratique sous certaines conditions) que la descente de gradient (typiquement linéaire)
	
\end{document}